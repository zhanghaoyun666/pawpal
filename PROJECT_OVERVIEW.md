# PawPal 项目完全指南

> **版本**: v1.5  
> **更新日期**: 2026-02-06  
> **状态**: 已开发完成 | 测试验证阶段  
> 
> 专为面试准备的项目简介，包含功能说明、技术实现和常见问题解答

---

## 一、项目一句话介绍

**PawPal** 是一个基于 React + FastAPI 的宠物领养平台，我主要负责设计和实现了三大AI功能：智能问卷、智能匹配、AI预审，通过LLM和Embedding技术提升领养匹配效率和成功率。

---

## 二、核心功能与技术实现

### 功能1: 智能问卷 (AI Questionnaire)

#### 功能说明
用户通过自然对话完成20维画像收集，替代传统表单填写，提升用户体验和数据质量。

#### 技术实现

| 技术点 | 具体实现 | 你的角色 |
|--------|----------|----------|
| **对话管理** | 维护多轮对话状态，记录chat_history | 设计对话流程，确定6-8轮收集策略 |
| **LLM调用** | 使用美团LongCat模型生成问题 | 编写System Prompt，设计JSON输出格式 |
| **画像提取** | 对话完成后用LLM提取结构化数据 | 定义20维画像字段，设计提取Prompt |
| **Fallback机制** | LLM失败时切换到Mock问题库 | 设计降级策略，保证服务可用性 |

#### 核心代码逻辑
```python
# 伪代码 - 面试时可以手绘这个流程
async def get_next_question(chat_history, current_profile):
    # 1. 尝试LLM生成问题
    llm_result = await call_longcat(prompt)
    
    # 2. 如果LLM失败，使用Mock
    if not llm_result:
        return mock_questions[turn]
    
    # 3. 解析JSON响应
    return parse_json(llm_result)
```

#### 面试话术
> "智能问卷的核心挑战是**如何在自然对话中高效收集结构化数据**。我的方案是：
> 1. 用System Prompt约束LLM每次只问一个问题，并解释为什么问
> 2. 对话完成后用另一个Prompt提取20维画像
> 3. 设计了Mock Fallback保证LLM故障时服务可用"

---

### 功能2: 智能匹配 (AI Matching)

#### 功能说明
基于用户画像和宠物档案，计算多维度匹配得分，推荐Top-N最适合的宠物。

#### 技术实现

| 技术点 | 具体实现 | 你的角色 |
|--------|----------|----------|
| **匹配算法** | 加权评分: 0.4硬性 + 0.4软性 + 0.2历史 | 设计权重分配，确定评分维度 |
| **硬性规则** | 空间-体型、经验-难度等一票否决规则 | 基于业务经验定义7条硬性规则 |
| **Embedding** | BGE-Large模型计算语义相似度 | 设计画像文本化策略，优化匹配效果 |
| **结果解释** | 生成匹配理由和顾虑点 | 设计可解释性方案，提升用户信任 |

#### 核心算法公式
```
总体得分 = 0.4 × HardConstraintScore 
        + 0.4 × SoftPreferenceScore 
        + 0.2 × HistoricalScore

硬性规则示例:
- 小公寓只能养小型宠物 (tiny/small)
- 新手不能养难训练的宠物
- 每天<1小时不能养高运动需求宠物
```

#### 面试话术
> "匹配算法的核心是**多维度加权评分**。我设计了三个层次：
> 1. **硬性条件**一票否决，比如小公寓不能养大型犬
> 2. **软性偏好**用Embedding计算语义相似度，比如'喜欢安静'和'性格温顺'的匹配
> 3. **历史成功率**作为辅助，冷启动时用默认值
> 
> 权重0.4:0.4:0.2是基于业务重要性确定的，硬性条件最重要"

---

### 功能3: AI预审 (AI Precheck)

#### 功能说明
替代人工审核中的基础资质核实，通过多轮对话自动识别20个高频风险点。

#### 技术实现

| 技术点 | 具体实现 | 你的角色 |
|--------|----------|----------|
| **状态机** | 10个状态的FSM管理对话流程 | 设计状态流转逻辑，定义每个状态的职责 |
| **风险检测** | 20个风险点的规则引擎 | 基于行业经验定义风险点和检测逻辑 |
| **风险澄清** | 发现风险后追问澄清 | 设计澄清对话策略，评估解释充分性 |
| **评分系统** | 100分制，≥80通过，<60拒绝 | 设计评分规则，确定通过阈值 |

#### 状态机设计
```
INIT → BASIC_INFO → HOUSING_CHECK → INCOME_CHECK → TIME_COMMITMENT
→ EXPERIENCE_CHECK → FAMILY_CHECK → MOTIVATION_CHECK → PREPARATION_CHECK
→ [RISK_CLARIFICATION] → SUMMARY → COMPLETE
```

#### 面试话术
> "AI预审用**状态机**管理复杂的对话流程。核心设计：
> 1. 定义了10个状态，每个状态收集特定信息
> 2. 内置20个风险检测规则，如'租房无许可''经常出差'
> 3. 发现风险后进入澄清状态，给用户解释机会
> 4. 最终100分制评分，替代70%的人工审核工作"

---

## 三、技术栈总览

### 前端
| 技术 | 用途 | 你实际用到的 |
|------|------|-------------|
| React 19 | UI框架 | 函数组件、Hooks |
| TypeScript | 类型安全 | 定义接口类型 |
| Tailwind CSS | 样式 | 原子化CSS类 |
| React Router | 路由 | 页面跳转 |

### 后端
| 技术 | 用途 | 你实际用到的 |
|------|------|-------------|
| FastAPI | Web框架 | API路由、Pydantic模型 |
| Supabase | 数据库 | PostgreSQL操作 |
| LongCat | LLM | 对话生成、画像提取 |
| BGE-Large | Embedding | 文本向量化 |

### AI/算法
| 技术 | 用途 | 核心参数 |
|------|------|----------|
| LLM Prompt工程 | 控制模型输出 | temperature=0.7, JSON模式 |
| Embedding相似度 | 语义匹配 | cosine similarity |
| 规则引擎 | 硬性条件检查 | 7条一票否决规则 |
| 状态机 | 对话流程管理 | 10个状态 |

---

## 四、项目亮点（面试加分项）

### 1. 鲁棒性设计
- **Mock Fallback**: LLM故障时自动降级，保证服务可用
- **规则兜底**: Embedding失败时用简化规则匹配
- **输入校验**: 特殊字符、超长输入的安全处理

### 2. 可解释性
- 每个匹配结果都有理由和顾虑说明
- 问卷每个问题都解释"为什么问这个"
- 预审结果展示各维度评估

### 3. 产品思维
- 20维画像设计基于真实领养场景
- 硬性规则来自行业经验和用户调研
- 权重分配考虑业务优先级

---

## 五、常见问题与回答

### Q1: 为什么选择LongCat而不是GPT-4?

**回答思路**: 成本 + 效果 + 合规
> "主要从三个角度考虑：
> 1. **成本**: LongCat是国内模型，价格更可控
> 2. **效果**: 经过测试，在中文对话场景下效果足够
> 3. **合规**: 国内部署，数据不出境
> 
> 当然架构上是解耦的，通过配置可以切换其他模型"

### Q2: 匹配算法的权重是怎么确定的?

**回答思路**: 业务逻辑 + 快速迭代
> "权重0.4:0.4:0.2是基于业务逻辑初步确定的：
> - 硬性条件最重要，不满足会直接导致领养失败
> - 软性偏好影响长期相处质量
> - 历史数据是辅助参考
> 
> 上线后计划通过A/B测试和数据反馈持续优化"

### Q3: 如何保证LLM输出的稳定性?

**回答思路**: Prompt工程 + 后处理 + Fallback
> "三个层面的保障：
> 1. **Prompt层面**: System Prompt明确约束输出格式，用JSON模式
> 2. **后处理层面**: 对LLM输出做校验，字段缺失时补默认值
> 3. **系统层面**: LLM失败时切换到Mock数据，保证用户体验"

### Q4: 20个风险点是怎么确定的?

**回答思路**: 行业经验 + 数据分析
> "主要来源：
> 1. **行业经验**: 参考宠物救助机构的审核标准
> 2. **数据分析**: 分析历史退养案例的共性原因
> 3. **用户调研**: 访谈协调员了解实际审核中的关注点
> 
> 比如'租房无许可''经常出差'都是高频导致弃养的因素"

### Q5: 如果LLM响应慢怎么办?

**回答思路**: 异步 + 缓存 + 体验优化
> "优化策略：
> 1. **技术层面**: LLM调用是异步的，不阻塞主线程
> 2. **缓存层面**: 相似问题可以缓存结果
> 3. **体验层面**: 加载动画让用户感知进度
> 4. **降级层面**: 超时后自动使用Mock数据"

### Q6: 你怎么评估AI功能的效果?

**回答思路**: 指标体系 + 数据闭环
> "建立了完整的指标体系：
> - **过程指标**: 问卷完成率、推荐点击率
> - **结果指标**: 申请通过率、最终领养成功率
> - **效率指标**: 人工审核工作量降低比例
> 
> 同时设计了埋点系统，可以追踪全链路数据"

---

## 六、项目结构速查

```
pawpal/
├── 前端 (React + TypeScript)
│   ├── pages/
│   │   ├── AIQuestionnaire.tsx      # 智能问卷页面
│   │   ├── AIRecommendations.tsx    # 匹配推荐页面
│   │   └── ...
│   ├── context/AppContext.tsx       # 全局状态管理
│   └── services/aiApi.ts            # AI相关API封装
│
├── 后端 (FastAPI + Python)
│   └── app/
│       ├── routers/
│       │   └── ai_v2.py             # AI功能API路由
│       └── services/
│           ├── ai_service.py        # LLM调用封装
│           ├── matching_engine.py   # 匹配算法实现
│           └── precheck_engine.py   # 预审状态机
│
└── 文档
    └── docs/
        ├── AI_PRD.md                # 产品需求文档
        ├── AI_TECH_SPEC.md          # 技术规格文档
        └── ...
```

---

## 七、面试准备建议

### 必须能讲清楚的内容
1. ✅ 三大AI功能的业务流程
2. ✅ 匹配算法的权重设计和公式
3. ✅ 20维画像包含哪些字段
4. ✅ 预审状态机的设计
5. ✅ 如何处理LLM不稳定

### 可以深入的技术点
1. 📌 Prompt工程技巧（System Prompt设计）
2. 📌 Embedding相似度计算原理
3. 📌 规则引擎 vs 模型预测的选择
4. 📌 冷启动和数据闭环设计

### 展示产品思维的问题
1. 💡 为什么用对话而不是表单？
2. 💡 如何平衡匹配精准度和多样性？
3. 💡 怎么让用户信任AI的推荐？
4. 💡 如果数据不好你会怎么优化？

---

**最后提醒**: 面试时如果遇到不会的问题，诚实地说"这个点我当时没有深入考虑，如果让我重新设计，我会..." 然后给出合理的思路，比硬编更好。

---

## 八、文档导航

- **详细产品文档**: [docs/AI_PRD.md](./docs/AI_PRD.md)
- **技术规格文档**: [docs/AI_TECH_SPEC.md](./docs/AI_TECH_SPEC.md)
- **文档总索引**: [docs/README.md](./docs/README.md)

---

**版本**: v1.5 | **状态**: 已开发完成 | 测试验证阶段

祝你面试顺利！🎉
