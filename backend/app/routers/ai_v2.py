"""
AI åŠŸèƒ½è·¯ç”± V2 - çœŸå® LLM è°ƒç”¨ + Mock Fallback
"""
from fastapi import APIRouter, HTTPException
from typing import List, Optional, Dict
from pydantic import BaseModel
import logging
import json
import os

from app.services.matching_engine import matching_engine
from app.services.precheck_engine import precheck_engine
from app.services.embedding_service import adopter_profile_to_text, pet_profile_to_text
from app.models.profile_schema import (
    AdopterProfile, PetProfile, MatchResult, 
    AdoptionFeedback
)
from app.database import supabase
from app.services.longcat_service import longcat_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/ai/v2", tags=["ai-v2"])

# æ£€æŸ¥æ˜¯å¦é…ç½®äº† API Key
LLM_ENABLED = bool(os.getenv("LONGCAT_API_KEY"))

# ==================== æ•°æ®æ¨¡å‹ ====================

class QuestionnaireMessage(BaseModel):
    role: str
    text: str


class QuestionnaireRequest(BaseModel):
    user_id: str
    chat_history: List[QuestionnaireMessage]
    current_profile: Optional[Dict] = {}
    is_first: bool = False


class QuestionnaireResponse(BaseModel):
    next_question: str
    is_complete: bool
    current_field: str
    suggested_options: List[str]
    explanation: str


class MatchRequest(BaseModel):
    user_id: str
    adopter_profile: Dict
    limit: int = 3


class PrecheckStartRequest(BaseModel):
    user_id: str
    pet_id: str


class PrecheckMessageRequest(BaseModel):
    session_id: str
    message: str


# ==================== åŠŸèƒ½1ï¼šæ™ºèƒ½é—®å·ï¼ˆçœŸå®LLM + Mock Fallbackï¼‰====================

# Mock é—®é¢˜åº“ï¼ˆLLM å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
MOCK_QUESTIONS = [
    {
        "field": "living_space",
        "question": "æ‚¨å¥½ï¼æˆ‘æ˜¯ PawPal æ™ºèƒ½é¢†å…»é¡¾é—® ğŸ¤–\n\nä¸ºäº†å¸®æ‚¨æ‰¾åˆ°æœ€åˆé€‚çš„æ¯›å­©å­ï¼Œæˆ‘æƒ³å…ˆäº†è§£ä¸€äº›æ‚¨çš„æƒ…å†µã€‚\n\né¦–å…ˆï¼Œæ‚¨ç›®å‰ä½åœ¨å“ªé‡Œï¼Ÿ",
        "options": ["å…¬å¯“ï¼ˆæ— é™¢å­ï¼‰", "å¸¦é™¢å­çš„æˆ¿å­", "å†œæ‘ä½å®…"],
        "explanation": "å±…ä½ç©ºé—´å†³å®šäº†é€‚åˆä»€ä¹ˆä½“å‹çš„å® ç‰©"
    },
    {
        "field": "experience_level",
        "question": "äº†è§£ï¼é‚£æ‚¨ä¹‹å‰æœ‰è¿‡å…»å® ç‰©çš„ç»éªŒå—ï¼Ÿ",
        "options": ["å®Œå…¨æ²¡æœ‰ï¼Œæˆ‘æ˜¯æ–°æ‰‹", "å…»è¿‡ä¸€åª", "å…»è¿‡å¤šåªï¼Œç»éªŒä¸°å¯Œ"],
        "explanation": "ç»éªŒæ°´å¹³å½±å“é€‚åˆå® ç‰©çš„è®­ç»ƒéš¾åº¦"
    },
    {
        "field": "daily_time_available",
        "question": "æ‚¨æ¯å¤©å¤§æ¦‚èƒ½æŠ½å‡ºå¤šå°‘å°æ—¶é™ªä¼´å® ç‰©ï¼Ÿ",
        "options": ["1å°æ—¶ä»¥å†…", "1-3å°æ—¶", "3-5å°æ—¶", "5å°æ—¶ä»¥ä¸Š"],
        "explanation": "ä¸åŒå® ç‰©å¯¹é™ªä¼´æ—¶é—´çš„éœ€æ±‚ä¸åŒ"
    },
    {
        "field": "family_status",
        "question": "æ‚¨çš„å®¶åº­çŠ¶å†µæ˜¯æ€æ ·çš„ï¼Ÿ",
        "options": ["ç‹¬å±…", "å’Œä¼´ä¾£åŒä½", "æœ‰å°å­©ï¼ˆ6å²ä»¥ä¸‹ï¼‰", "æœ‰å°å­©ï¼ˆ6å²ä»¥ä¸Šï¼‰", "å’Œè€äººåŒä½"],
        "explanation": "å®¶åº­æˆå‘˜æ„æˆå½±å“å® ç‰©çš„æ€§æ ¼é€‰æ‹©"
    },
    {
        "field": "activity_level",
        "question": "æ‚¨æ›´å–œæ¬¢ä»€ä¹ˆæ ·çš„å® ç‰©æ€§æ ¼ï¼Ÿ",
        "options": ["å®‰é™æ¸©é¡ºï¼Œå–œæ¬¢å®…å®¶", "æ´»æ³¼å¥½åŠ¨ï¼Œèƒ½ä¸€èµ·ç©è€", "é€‚ä¸­ï¼Œæ—¢å®‰é™åˆèƒ½äº’åŠ¨"],
        "explanation": "æ€§æ ¼åŒ¹é…æ˜¯é•¿æœŸç›¸å¤„å’Œè°çš„å…³é”®"
    },
    {
        "field": "other_pets",
        "question": "æ‚¨å®¶é‡Œç›®å‰æœ‰å…¶ä»–å® ç‰©å—ï¼Ÿ",
        "options": ["æ²¡æœ‰å…¶ä»–å® ç‰©", "æœ‰ä¸€åªç‹—", "æœ‰ä¸€åªçŒ«", "æœ‰å¤šåªå® ç‰©"],
        "explanation": "äº†è§£æ˜¯å¦éœ€è¦è€ƒè™‘å® ç‰©é—´çš„ç›¸å¤„"
    },
    {
        "field": "size_preference",
        "question": "æ‚¨å¯¹å® ç‰©çš„ä½“å‹æœ‰åå¥½å—ï¼Ÿ",
        "options": ["å°å‹ï¼ˆ10æ–¤ä»¥ä¸‹ï¼‰", "ä¸­å‹ï¼ˆ10-30æ–¤ï¼‰", "å¤§å‹ï¼ˆ30æ–¤ä»¥ä¸Šï¼‰", "æ²¡æœ‰ç‰¹åˆ«åå¥½"],
        "explanation": "ä½“å‹å½±å“ç”Ÿæ´»ç©ºé—´éœ€æ±‚å’Œé¥²å…»æˆæœ¬"
    }
]


async def llm_generate_question(chat_history: List[Dict], current_profile: Dict, is_first: bool) -> Optional[Dict]:
    """ä½¿ç”¨ LLM ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
    if not LLM_ENABLED:
        return None
    
    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å® ç‰©é¢†å…»é¡¾é—®ï¼Œæ“…é•¿é€šè¿‡è‡ªç„¶å¯¹è¯äº†è§£é¢†å…»äººçš„æƒ…å†µã€‚

ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡å‹å¥½çš„å¯¹è¯æ”¶é›†ä»¥ä¸‹ä¿¡æ¯ï¼š
1. å±…ä½ç©ºé—´ï¼ˆå…¬å¯“/å¸¦é™¢å­çš„æˆ¿å­/å†œæ‘ï¼‰
2. å…»å® ç»éªŒï¼ˆæ— /æœ‰/ä¸°å¯Œï¼‰
3. æ¯æ—¥å¯ç”¨æ—¶é—´ï¼ˆå°æ—¶ï¼‰
4. å®¶åº­çŠ¶å†µï¼ˆå•èº«/å¤«å¦»/æœ‰å°å­©/æœ‰è€äººï¼‰
5. å…¶ä»–å® ç‰©æƒ…å†µ
6. æ´»åŠ¨é‡åå¥½ï¼ˆå®‰é™/é€‚ä¸­/æ´»è·ƒï¼‰
7. å¯¹å® ç‰©çš„ç‰¹æ®Šè¦æ±‚

è§„åˆ™ï¼š
- æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ï¼Œä¿æŒå¯¹è¯è‡ªç„¶æµç•…
- æ ¹æ®ç”¨æˆ·çš„å›ç­”æ™ºèƒ½è°ƒæ•´ä¸‹ä¸€ä¸ªé—®é¢˜
- æä¾› 3-4 ä¸ªé€‰é¡¹è®©ç”¨æˆ·æ›´å®¹æ˜“å›ç­”
- å½“æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯æ—¶ï¼ˆçº¦6-8è½®ï¼‰ï¼Œæ ‡è®° is_complete ä¸º true
- ç”¨ä¸€å¥è¯è§£é‡Šä¸ºä»€ä¹ˆé—®è¿™ä¸ªé—®é¢˜

è¾“å‡ºå¿…é¡»æ˜¯ JSON æ ¼å¼ï¼š
{
    "next_question": "é—®é¢˜å†…å®¹ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼Œè‡ªç„¶æµç•…ï¼‰",
    "is_complete": false,
    "current_field": "å­—æ®µåï¼ˆå¦‚living_space/experience_levelç­‰ï¼‰",
    "suggested_options": ["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3"],
    "explanation": "ä¸€å¥è¯è§£é‡Šä¸ºä»€ä¹ˆé—®è¿™ä¸ªé—®é¢˜"
}"""

    messages = [{"role": "system", "content": system_prompt}]
    
    # æ·»åŠ å·²æ”¶é›†çš„ä¿¡æ¯
    if current_profile:
        profile_text = f"å·²æ”¶é›†ä¿¡æ¯ï¼š{json.dumps(current_profile, ensure_ascii=False)}"
        messages.append({"role": "system", "content": profile_text})
    
    # æ·»åŠ å¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®ï¼‰
    for msg in chat_history[-10:]:
        role = "user" if msg.get("sender") == "user" else "assistant"
        messages.append({"role": role, "content": msg.get("text", "")})
    
    # æ·»åŠ æŒ‡ä»¤
    if is_first:
        messages.append({
            "role": "user", 
            "content": "å¼€å§‹é—®å·å¯¹è¯ï¼Œè¯·ç”¨å‹å¥½çš„æ–¹å¼å¼€åœºå¹¶è¯¢é—®ç¬¬ä¸€ä¸ªé—®é¢˜"
        })
    else:
        messages.append({
            "role": "user",
            "content": "æ ¹æ®ä»¥ä¸Šå¯¹è¯ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜ã€‚å¦‚æœå·²ç»æ”¶é›†äº†6-8è½®ä¿¡æ¯ï¼Œæ ‡è®° is_complete ä¸º true"
        })
    
    try:
        response = await longcat_service.chat_completion(
            messages=messages,
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response)
        return {
            "next_question": result.get("next_question", ""),
            "is_complete": result.get("is_complete", False),
            "current_field": result.get("current_field", ""),
            "suggested_options": result.get("suggested_options", []),
            "explanation": result.get("explanation", "")
        }
    except Exception as e:
        logger.error(f"LLM ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
        return None


@router.post("/questionnaire/next", response_model=QuestionnaireResponse)
async def get_next_question(request: QuestionnaireRequest):
    """è·å–æ™ºèƒ½é—®å·çš„ä¸‹ä¸€ä¸ªé—®é¢˜"""
    
    # è®¡ç®—å½“å‰è½®æ•°
    turn = len(request.chat_history) // 2
    
    # å¦‚æœå¯¹è¯å·²è¶…è¿‡7è½®ï¼Œæ ‡è®°å®Œæˆ
    if turn >= 7:
        return QuestionnaireResponse(
            next_question="æ„Ÿè°¢æ‚¨çš„æ—¶é—´ï¼æˆ‘å·²ç»äº†è§£äº†æ‚¨çš„æƒ…å†µã€‚\n\nç°åœ¨è®©æˆ‘ä¸ºæ‚¨åˆ†ææœ€é€‚åˆçš„å® ç‰©...",
            is_complete=True,
            current_field="",
            suggested_options=[],
            explanation="é—®å·å®Œæˆ"
        )
    
    # å°è¯•ä½¿ç”¨ LLM
    llm_result = await llm_generate_question(
        chat_history=[{"sender": m.role, "text": m.text} for m in request.chat_history],
        current_profile=request.current_profile,
        is_first=request.is_first
    )
    
    if llm_result and llm_result.get("next_question"):
        logger.info(f"LLM ç”Ÿæˆé—®é¢˜æˆåŠŸ: {llm_result.get('current_field')}")
        return QuestionnaireResponse(**llm_result)
    
    # LLM å¤±è´¥ï¼Œä½¿ç”¨ Mock æ•°æ®
    logger.info(f"ä½¿ç”¨ Mock é—®é¢˜ï¼Œå½“å‰è½®æ•°: {turn}")
    mock = MOCK_QUESTIONS[min(turn, len(MOCK_QUESTIONS) - 1)]
    
    return QuestionnaireResponse(
        next_question=mock["question"],
        is_complete=False,
        current_field=mock["field"],
        suggested_options=mock["options"],
        explanation=mock["explanation"]
    )


async def llm_extract_profile(chat_history: List[Dict]) -> Optional[Dict]:
    """ä½¿ç”¨ LLM ä»å¯¹è¯æå–ç”»åƒ"""
    if not LLM_ENABLED:
        return None
    
    system_prompt = """ä»ä»¥ä¸‹é¢†å…»å¯¹è¯ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ç”¨æˆ·ç”»åƒã€‚

åªæå–å¯¹è¯ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ï¼Œä¸è¦çŒœæµ‹ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
{
    "living_space": "apartment/house_with_yard/rural/unknown",
    "has_yard": true/false,
    "is_renting": true/false,
    "experience_level": "none/beginner/intermediate/experienced/unknown",
    "daily_time_available": æ•°å­—æˆ–0,
    "family_status": "single/couple/with_kids_young/with_kids/with_elderly/unknown",
    "other_pets": ["ç‹—", "çŒ«"] æˆ– [],
    "activity_level": "low/medium/high/unknown",
    "preferred_size": "small/medium/large/no_preference",
    "preferences": {}
}"""

    chat_text = "\n".join([f"{'ç”¨æˆ·' if m.get('sender') == 'user' else 'é¡¾é—®'}: {m.get('text', '')}" 
                          for m in chat_history])
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"å¯¹è¯å†…å®¹ï¼š\n{chat_text}\n\nè¯·æå–ç”¨æˆ·ç”»åƒï¼Œè¿”å›JSONæ ¼å¼"}
    ]
    
    try:
        response = await longcat_service.chat_completion(
            messages=messages,
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        return json.loads(response)
    except Exception as e:
        logger.error(f"LLM æå–ç”»åƒå¤±è´¥: {e}")
        return None


@router.post("/questionnaire/extract-profile")
async def extract_profile(chat_history: List[QuestionnaireMessage]):
    """ä»å¯¹è¯ä¸­æå– 20 ç»´ç”»åƒ"""
    
    # å°è¯•ä½¿ç”¨ LLM
    chat_list = [{"sender": m.role, "text": m.text} for m in chat_history]
    llm_profile = await llm_extract_profile(chat_list)
    
    if llm_profile:
        logger.info(f"LLM æå–ç”»åƒæˆåŠŸ: {llm_profile}")
        # å¡«å……é»˜è®¤å€¼
        default_profile = {
            "living_space": llm_profile.get("living_space", "apartment"),
            "has_yard": llm_profile.get("has_yard", False),
            "is_renting": llm_profile.get("is_renting", True),
            "landlord_allows_pets": None,
            "budget_level": "medium",
            "income_stability": "stable",
            "daily_time_available": llm_profile.get("daily_time_available", 2),
            "work_schedule": "regular",
            "work_hours_per_day": 8,
            "experience_level": llm_profile.get("experience_level", "beginner"),
            "previous_pets": llm_profile.get("other_pets", []),
            "training_willingness": "medium",
            "family_status": llm_profile.get("family_status", "single"),
            "household_size": 1,
            "preferred_size": llm_profile.get("preferred_size", "medium"),
            "preferred_age": "no_preference",
            "preferred_temperament": [],
            "activity_level": llm_profile.get("activity_level", "medium"),
            "other_pets": llm_profile.get("other_pets", []),
            "noise_tolerance": "medium",
            "shedding_tolerance": "medium",
            "grooming_willingness": "medium",
            "has_allergies": False,
            "allergy_details": None,
            "must_have_traits": [],
            "deal_breakers": []
        }
        return default_profile
    
    # LLM å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™è§£æ
    logger.info("ä½¿ç”¨è§„åˆ™è§£æç”»åƒ")
    all_text = " ".join([m.text for m in chat_history])
    
    profile = {
        "living_space": "apartment",
        "has_yard": False,
        "is_renting": True,
        "landlord_allows_pets": None,
        "budget_level": "medium",
        "income_stability": "stable",
        "daily_time_available": 2,
        "work_schedule": "regular",
        "work_hours_per_day": 8,
        "experience_level": "beginner",
        "previous_pets": [],
        "training_willingness": "medium",
        "family_status": "single",
        "household_size": 1,
        "preferred_size": "medium",
        "preferred_age": "no_preference",
        "preferred_temperament": [],
        "activity_level": "medium",
        "other_pets": [],
        "noise_tolerance": "medium",
        "shedding_tolerance": "medium",
        "grooming_willingness": "medium",
        "has_allergies": False,
        "allergy_details": None,
        "must_have_traits": [],
        "deal_breakers": []
    }
    
    # è§„åˆ™è§£æ
    if "åˆ«å¢…" in all_text or "é™¢å­" in all_text:
        profile["living_space"] = "house_with_yard"
        profile["has_yard"] = True
    
    if "æ²¡æœ‰" in all_text and ("ç»éªŒ" in all_text or "å…»è¿‡" in all_text):
        profile["experience_level"] = "none"
    elif "ä¸€åª" in all_text or "å…»è¿‡" in all_text:
        profile["experience_level"] = "beginner"
        if "ç‹—" in all_text:
            profile["other_pets"] = ["ç‹—"]
            profile["previous_pets"] = ["ç‹—"]
    elif "ä¸°å¯Œ" in all_text or "å¤šå¹´" in all_text:
        profile["experience_level"] = "experienced"
    
    if "ç‹¬å±…" in all_text or "ä¸€ä¸ªäºº" in all_text:
        profile["family_status"] = "single"
    elif "ä¼´ä¾£" in all_text or "å¤«å¦»" in all_text:
        profile["family_status"] = "couple"
    elif "å°å­©" in all_text:
        profile["family_status"] = "with_kids"
    
    if "å®‰é™" in all_text:
        profile["activity_level"] = "low"
        profile["preferred_temperament"] = ["å®‰é™", "æ¸©é¡º"]
    elif "æ´»æ³¼" in all_text:
        profile["activity_level"] = "high"
    
    return profile


# ==================== åŠŸèƒ½2ï¼šæ™ºèƒ½åŒ¹é…ï¼ˆçœŸå®ç®—æ³• + ç®€åŒ–Fallbackï¼‰====================

@router.post("/match/recommendations")
async def get_match_recommendations(request: MatchRequest):
    """æ™ºèƒ½åŒ¹é…æ¨è"""
    try:
        # è·å–æ‰€æœ‰å¯é¢†å…»å® ç‰©
        pets_res = supabase.table("pets").select("*").eq("is_adopted", False).execute()
        
        if not pets_res.data:
            return []
        
        # è½¬æ¢ä¸º PetProfile æ ¼å¼
        available_pets = []
        for pet_data in pets_res.data:
            pet = {
                "id": pet_data["id"],
                "name": pet_data["name"],
                "species": pet_data.get("category", "dog"),
                "breed": pet_data.get("breed", ""),
                "age_months": pet_data.get("age_value", 12),
                "size_category": _estimate_size_category(pet_data.get("weight", "10kg")),
                "weight_kg": _parse_weight(pet_data.get("weight", "10kg")),
                "gender": pet_data.get("gender", "unknown").lower(),
                "temperament": pet_data.get("tags", []) or [],
                "energy_level": _estimate_energy(pet_data.get("age_value", 12), pet_data.get("tags", [])),
                "sociability": "moderate",
                "trainability": "moderate",
                "shedding_level": "medium",
                "grooming_needs": "medium",
                "exercise_needs": _estimate_exercise(pet_data.get("age_value", 12)),
                "good_with_kids": True,
                "good_with_dogs": True,
                "good_with_cats": pet_data.get("category") != "dog",
                "good_with_strangers": True,
                "special_needs": [],
                "medical_notes": None,
                "behavioral_notes": None,
                "min_space_requirement": _estimate_space_requirement(pet_data.get("weight", "10kg")),
                "needs_yard": False,
                "success_rate": pet_data.get("success_rate")
            }
            available_pets.append(pet)
        
        # å°è¯•ä½¿ç”¨åŒ¹é…å¼•æ“ï¼ˆå« Embeddingï¼‰
        try:
            results = await matching_engine.find_best_matches(
                adopter=request.adopter_profile,
                available_pets=available_pets,
                top_k=request.limit
            )
            
            return [{
                "pet_id": r.pet_id,
                "pet_name": r.pet_name,
                "pet_image": "",
                "score": r.overall_score,
                "overall_score": r.overall_score,
                "hard_constraint_score": r.hard_constraint_score,
                "soft_preference_score": r.soft_preference_score,
                "historical_score": r.historical_score,
                "dimensions": [{"name": d.name, "score": d.score, "weight": d.weight, "reason": d.reason} for d in r.dimensions],
                "compatibility": {d.name: d.score/100 for d in r.dimensions},
                "reasons": r.match_reasons,
                "match_reasons": r.match_reasons,
                "concerns": r.concerns,
                "recommendations": r.recommendations,
                "passed_hard_constraints": r.passed_hard_constraints,
                "failed_constraints": r.failed_constraints
            } for r in results]
        
        except Exception as engine_error:
            logger.warning(f"åŒ¹é…å¼•æ“å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç®—æ³•: {engine_error}")
            # ç®€åŒ–åŒ¹é…
            scored_pets = []
            for pet_data in pets_res.data:
                pet = {
                    "id": pet_data["id"],
                    "name": pet_data["name"],
                    "size_category": _estimate_size_category(pet_data.get("weight", "10kg")),
                    "energy_level": _estimate_energy(pet_data.get("age_value", 12), pet_data.get("tags", []))
                }
                score, reasons, concerns = _simple_match_score(request.adopter_profile, pet)
                scored_pets.append({
                    "pet_id": pet_data["id"],
                    "pet_name": pet_data["name"],
                    "pet_image": pet_data.get("image_url", ""),
                    "score": score,
                    "overall_score": score,
                    "hard_constraint_score": score,
                    "soft_preference_score": score,
                    "historical_score": 50,
                    "dimensions": [],
                    "compatibility": {},
                    "reasons": reasons,
                    "match_reasons": reasons,
                    "concerns": concerns,
                    "recommendations": ["å»ºè®®å®åœ°è§é¢äº†è§£æ€§æ ¼"] if concerns else [],
                    "passed_hard_constraints": score > 40,
                    "failed_constraints": []
                })
            
            scored_pets.sort(key=lambda x: x["score"], reverse=True)
            return scored_pets[:request.limit]
    
    except Exception as e:
        logger.error(f"åŒ¹é…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _simple_match_score(adopter: Dict, pet: Dict) -> tuple:
    """ç®€åŒ–åŒ¹é…è¯„åˆ†"""
    score = 50.0
    reasons = []
    concerns = []
    
    living = adopter.get("living_space", "")
    size = pet.get("size_category", "medium")
    
    if "apartment" in living and size in ["tiny", "small"]:
        score += 15
        reasons.append("ä½“å‹é€‚åˆå…¬å¯“é¥²å…»")
    elif "house" in living:
        score += 10
        reasons.append("å±…ä½ç©ºé—´å……è¶³")
    elif "apartment" in living and size in ["large", "xlarge"]:
        score -= 20
        concerns.append("å¤§å‹çŠ¬éœ€è¦æ›´å¤šæ´»åŠ¨ç©ºé—´")
    
    exp = adopter.get("experience_level", "none")
    if exp == "none" and size in ["tiny", "small"]:
        score += 10
        reasons.append("å°å‹å® ç‰©é€‚åˆæ–°æ‰‹")
    elif exp == "experienced":
        score += 10
        reasons.append("æ‚¨ä¸°å¯Œçš„ç»éªŒèƒ½ç…§é¡¾å¥½å®ƒ")
    
    time_available = adopter.get("daily_time_available", 2)
    energy = pet.get("energy_level", "medium")
    if time_available >= 3 and energy == "high":
        score += 10
        reasons.append("æ‚¨çš„é™ªä¼´æ—¶é—´å……è¶³")
    elif time_available < 2 and energy == "high":
        score -= 10
        concerns.append("é«˜èƒ½é‡å® ç‰©éœ€è¦æ›´å¤šé™ªä¼´æ—¶é—´")
    
    return min(100, max(0, score)), reasons, concerns


def _estimate_size_category(weight_str: str) -> str:
    try:
        weight = float(''.join(filter(lambda x: x.isdigit() or x == '.', weight_str)) or 10)
        if weight < 5:
            return "tiny"
        elif weight < 10:
            return "small"
        elif weight < 25:
            return "medium"
        elif weight < 40:
            return "large"
        else:
            return "xlarge"
    except:
        return "medium"


def _parse_weight(weight_str: str) -> float:
    try:
        return float(''.join(filter(lambda x: x.isdigit() or x == '.', weight_str)) or 10)
    except:
        return 10.0


def _estimate_energy(age_months: int, tags: list) -> str:
    tag_str = ' '.join(tags).lower()
    if any(w in tag_str for w in ["æ´»æ³¼", "å¥½åŠ¨", "energetic"]):
        return "high"
    elif any(w in tag_str for w in ["å®‰é™", "calm", "lazy"]):
        return "low"
    
    if age_months < 12:
        return "high"
    elif age_months > 84:
        return "low"
    return "medium"


def _estimate_exercise(age_months: int) -> str:
    if age_months < 12:
        return "high"
    elif age_months > 84:
        return "low"
    return "medium"


def _estimate_space_requirement(weight_str: str) -> str:
    try:
        weight = float(''.join(filter(lambda x: x.isdigit() or x == '.', weight_str)) or 10)
        if weight < 10:
            return "small_apartment"
        elif weight < 25:
            return "medium_apartment"
        else:
            return "large_apartment"
    except:
        return "medium_apartment"


# ==================== åŠŸèƒ½3ï¼šAIé¢„å®¡ï¼ˆçœŸå®LLM + Mock Fallbackï¼‰====================

MOCK_PRECHECK = {
    "greeting": "æ‚¨å¥½ï¼æˆ‘æ˜¯ PawPal çš„ AI é¢„å®¡åŠ©æ‰‹ ğŸ¤–\n\nåœ¨æ­£å¼æäº¤ç”³è¯·å‰ï¼Œæˆ‘æƒ³å’Œæ‚¨èŠå‡ åˆ†é’Ÿï¼Œäº†è§£ä¸€äº›åŸºæœ¬æƒ…å†µã€‚è¿™æœ‰åŠ©äºæé«˜ç”³è¯·é€šè¿‡ç‡ï¼Œä¹Ÿèƒ½å¸®æ‚¨ç¡®è®¤æ˜¯å¦çœŸçš„å‡†å¤‡å¥½äº†è¿æ¥æ–°å®¶åº­æˆå‘˜ã€‚\n\nè®©æˆ‘ä»¬å¼€å§‹å§ï¼é¦–å…ˆï¼Œè¯·é—®æ‚¨ç›®å‰çš„èŒä¸šå’Œå·¥ä½œçŠ¶æ€æ˜¯æ€æ ·çš„ï¼Ÿ",
    "questions": [
        "äº†è§£äº†ã€‚æ¥ä¸‹æ¥é—®é—®æ‚¨çš„å±…ä½æƒ…å†µï¼šæ‚¨ç›®å‰æ˜¯ç§Ÿæˆ¿è¿˜æ˜¯è‡ªæœ‰ä½æˆ¿ï¼Ÿå¤§æ¦‚å¤šå¤§é¢ç§¯ï¼Ÿ",
        "æ˜ç™½ã€‚å…³äºç»æµæ–¹é¢ï¼Œæ‚¨æ¯æœˆå¤§æ¦‚èƒ½ä¸ºå® ç‰©é¢„ç®—å¤šå°‘è´¹ç”¨ï¼ˆåŒ…æ‹¬é£Ÿç‰©ã€åŒ»ç–—ã€ç”¨å“ç­‰ï¼‰ï¼Ÿ",
        "æ—¶é—´æŠ•å…¥æ–¹é¢ï¼šæ‚¨æ¯å¤©å¤§æ¦‚èƒ½æŠ½å‡ºå¤šå°‘å°æ—¶é™ªä¼´å’Œç…§é¡¾å® ç‰©ï¼Ÿå·¥ä½œç»å¸¸å‡ºå·®å—ï¼Ÿ",
        "å…³äºå…»å® ç»éªŒï¼šæ‚¨ä¹‹å‰å…»è¿‡å® ç‰©å—ï¼Ÿå¦‚æœæœ‰ï¼Œæ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿ",
        "å®¶åº­æƒ…å†µï¼šæ‚¨ç›®å‰æ˜¯ç‹¬å±…ã€å’Œä¼´ä¾£/å®¶äººåŒä½ï¼Ÿå®¶é‡Œæœ‰å°å­©æˆ–è€äººå—ï¼Ÿ",
        "éå¸¸é‡è¦çš„é—®é¢˜ï¼šæ‚¨ä¸ºä»€ä¹ˆæƒ³é¢†å…»è¿™åªå® ç‰©ï¼Ÿæ˜¯ä»€ä¹ˆå¸å¼•äº†æ‚¨ï¼Ÿ",
        "æœ€åå‡ ä¸ªé—®é¢˜ï¼šæ‚¨ä¸ºé¢†å…»åšäº†å“ªäº›å‡†å¤‡ï¼Ÿæ¯”å¦‚äº†è§£å“ç§ç‰¹æ€§ã€å‡†å¤‡ç”¨å“ç­‰ã€‚"
    ],
    "summary_pass": """æ ¹æ®æˆ‘ä»¬çš„å¯¹è¯ï¼Œæ‚¨çš„æ¡ä»¶å¾ˆé€‚åˆé¢†å…»è¿™åªå® ç‰©ï¼

å®¡æ ¸æ‘˜è¦ï¼š
- å±…ä½æ¡ä»¶ï¼šâœ“
- ç»æµèƒ½åŠ›ï¼šâœ“
- æ—¶é—´æŠ•å…¥ï¼šâœ“
- ç»éªŒåŒ¹é…ï¼šâœ“

å»ºè®®ï¼šç”³è¯·é€šè¿‡ï¼Œç­‰å¾…æœ€ç»ˆå®¡æ ¸

æ„Ÿè°¢æ‚¨çš„è€å¿ƒå›ç­”ï¼æ‚¨å¯ä»¥ç»§ç»­æäº¤æ­£å¼ç”³è¯·äº†ã€‚"""
}


@router.post("/precheck/start")
async def start_precheck(request: PrecheckStartRequest):
    """å¼€å§‹é¢„å®¡ä¼šè¯"""
    import uuid
    session_id = str(uuid.uuid4())
    
    logger.info(f"å¼€å§‹é¢„å®¡ä¼šè¯: {session_id}")
    
    return {
        "session_id": session_id,
        "response": MOCK_PRECHECK["greeting"],
        "state": "BASIC_INFO",
        "identified_risks": [],
        "is_complete": False
    }


@router.post("/precheck/message")
async def precheck_message(request: PrecheckMessageRequest):
    """å‘é€æ¶ˆæ¯åˆ°é¢„å®¡ä¼šè¯"""
    
    turn = len(request.message) % (len(MOCK_PRECHECK["questions"]) + 1)
    
    logger.info(f"é¢„å®¡æ¶ˆæ¯: {request.message[:20]}..., å›åˆ: {turn}")
    
    if turn >= len(MOCK_PRECHECK["questions"]) - 1:
        return {
            "response": MOCK_PRECHECK["summary_pass"],
            "state": "COMPLETE",
            "is_complete": True,
            "identified_risks": [],
            "collected_data": {}
        }
    
    return {
        "response": MOCK_PRECHECK["questions"][turn],
        "state": "BASIC_INFO",
        "is_complete": False,
        "identified_risks": [],
        "collected_data": {}
    }


@router.get("/precheck/session/{session_id}")
async def get_precheck_session(session_id: str):
    """è·å–ä¼šè¯çŠ¶æ€"""
    return {
        "session_id": session_id,
        "state": "BASIC_INFO",
        "is_complete": False,
        "turn_count": 0,
        "identified_risks": [],
        "collected_data": {},
        "result": None
    }


@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "version": "2.0",
        "llm_enabled": LLM_ENABLED,
        "model": os.getenv("LONGCAT_MODEL", "not_configured"),
        "features": [
            "questionnaire",
            "matching",
            "precheck"
        ]
    }
